{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Author Prediction - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 17:57:00.623982: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-11 17:57:00.626195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-11 17:57:00.881401: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 17:57:01.415420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 17:57:05.901276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "root = os.path.join(current_dir, \"Books\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through author and book folders, adding .txt file contents to data with matching labels\n",
    "for author in os.listdir(root):\n",
    "    author_path = os.path.join(root, author)\n",
    "\n",
    "    for book in os.listdir(author_path):\n",
    "        book_path = os.path.join(author_path, book)\n",
    "\n",
    "        for chapter in os.listdir(book_path):\n",
    "            chapter_path = os.path.join(book_path, chapter)\n",
    "            \n",
    "            with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "            data.append({\n",
    "                'Author': author,\n",
    "                'Book': book,\n",
    "                'Chapter': chapter[:-4],\n",
    "                'Text': text\n",
    "            })\n",
    "\n",
    "dataset = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "fragment_size = 200\n",
    "overlap = 50\n",
    "\n",
    "# First we clean our text, converting to lower case and removing unwanted characted\n",
    "# Then we apply corpus, simplifying our text\n",
    "# Finally we split our text into fragments of 'fragment_size', with an overlap of 'overlap' words from the previous fragment\n",
    "\n",
    "def preprocess_text(text, ps, all_stopwords):\n",
    "    # Clean text\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\n', \" \", text)  # Newlines\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', \" \", text)  # Punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Extra spaces\n",
    "\n",
    "    # Apply corpus\n",
    "    words = text.split()\n",
    "    words = [ps.stem(word) for word in words if word not in all_stopwords]\n",
    "    processed_text = \" \".join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def fragment_text(text, fragment_size, overlap):\n",
    "    # Split text into fragments of fragment_size length, returns array of fragments\n",
    "    words = text.split()\n",
    "    current_text_fragments = []\n",
    "    \n",
    "    step_size = fragment_size - overlap  \n",
    "    \n",
    "    for i in range(0, len(words), step_size):\n",
    "        current_fragment = \" \".join(words[i:i + fragment_size])\n",
    "        current_text_fragments.append(current_fragment)\n",
    "\n",
    "        # Handle situation where final chapter fragment is already contained in the previous fragment\n",
    "        if len(words) - i < fragment_size:\n",
    "            break\n",
    "        \n",
    "    return current_text_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lyons/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "nltk.download('stopwords')\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "# Apply our cleaning and create a new dataset to replace our previous one, this time with processed text\n",
    "text_fragments = []\n",
    "for index, row in dataset.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    text = preprocess_text(text, ps, all_stopwords)\n",
    "    current_text_fragments = fragment_text(text, fragment_size, overlap)\n",
    "    \n",
    "    for text_fragment in current_text_fragments:\n",
    "        text_fragments.append({\n",
    "            \"Book\": row[\"Book\"],\n",
    "            \"Author\": row[\"Author\"],\n",
    "            \"Text\": text_fragment\n",
    "        })\n",
    "\n",
    "# Convert the data fragments into a Pandas DataFrame and replace the original\n",
    "dataset = pd.DataFrame(text_fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"Text\"].values\n",
    "y = dataset[\"Author\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Reshape y to a 2D array (needed for OneHotEncoder)\n",
    "y_reshaped = y.reshape(-1, 1)\n",
    "\n",
    "# Create and fit OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Creating class weights (in order to reduce overfitting from imbalanced data size per author)\n",
    "y_class_indices = np.argmax(y_encoded, axis=-1)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(y_class_indices), \n",
    "    y=y_class_indices\n",
    ")\n",
    "\n",
    "# Creating class weights dictionary\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings after .strip() applied\n",
    "# There shouldn't be any but BERT tokeniser fails if not done\n",
    "X = [x for x in X if x.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Michael/Desktop/Web Development Projects/Personal Projects/Portfolio/python-server/linux_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-02-11 17:57:25.173910: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.313373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.313423: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.317312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.317400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.317432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.545304: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.545367: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.545374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-02-11 17:57:25.545401: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-11 17:57:25.545413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5592 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Max length chosen based on token distribution coming from fragment size to minimise truncation\n",
    "# Edit if changing fragment_size, or optionally remove  entirely though this should be less efficient\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_tokenized = tokenizer(\n",
    "    X_train, \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=330,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "X_test_tokenized = tokenizer(\n",
    "    X_test, \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=330,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pretrained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_model(learning_rate=2e-5, dropout_rate=0.3):\n",
    "    # Define input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(330,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(330,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Define output\n",
    "    bert_output = bert_model([input_ids, attention_mask])\n",
    "    pooled_output = bert_output.pooler_output\n",
    "    \n",
    "    # Add dropout (prevent overfitting)\n",
    "    dropout = Dropout(dropout_rate)(pooled_output)\n",
    "    \n",
    "    # Create classification layers\n",
    "    num_authors = len(dataset['Author'].unique())\n",
    "    output = Dense(num_authors, activation=\"softmax\")(dropout)\n",
    "    \n",
    "    # # Create model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Unfreeze only the last few layers (e.g., last 4)\n",
    "for layer in bert_model.layers[-4:]:  \n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# Create the model\n",
    "model = create_model(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # Monitor validation loss\n",
    "    patience=3,                 # Stop after 3 epochs with no improvement\n",
    "    restore_best_weights=True   # Restore model weights from the best epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 17:57:47.024389: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f378afeb500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-11 17:57:47.024417: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n",
      "2025-02-11 17:57:47.078586: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-11 17:57:47.142927: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739296667.215878    7946 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 85s 389ms/step - loss: 2.2284 - accuracy: 0.2631 - val_loss: 1.3338 - val_accuracy: 0.5169\n",
      "Epoch 2/6\n",
      "163/163 [==============================] - 61s 377ms/step - loss: 0.8375 - accuracy: 0.7738 - val_loss: 0.3044 - val_accuracy: 0.9415\n",
      "Epoch 3/6\n",
      "163/163 [==============================] - 59s 360ms/step - loss: 0.2949 - accuracy: 0.9331 - val_loss: 0.1881 - val_accuracy: 0.9538\n",
      "Epoch 4/6\n",
      "163/163 [==============================] - 62s 362ms/step - loss: 0.1431 - accuracy: 0.9754 - val_loss: 0.1153 - val_accuracy: 0.9723\n",
      "Epoch 5/6\n",
      "163/163 [==============================] - 62s 382ms/step - loss: 0.0657 - accuracy: 0.9900 - val_loss: 0.1093 - val_accuracy: 0.9631\n",
      "Epoch 6/6\n",
      "163/163 [==============================] - 62s 378ms/step - loss: 0.0385 - accuracy: 0.9954 - val_loss: 0.0925 - val_accuracy: 0.9692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f38867eff10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set epochs lower if not using early stopping, observed some good results at 5+\n",
    "# Converges at around 19 with seven authors, takes 1h30m on CPU\n",
    "# Lower batch size if running on GPU and you get an out of memory error, 8 seems to work for 8gb VRAM\n",
    "epochs = 6\n",
    "batch_size = 8\n",
    "\n",
    "# Label input ids and attention mask for BERT (from BERT tokenizer), for the sake of convienience\n",
    "train_input_ids = X_tokenized['input_ids']\n",
    "train_attention_mask = X_tokenized['attention_mask']\n",
    "test_input_ids = X_test_tokenized['input_ids']\n",
    "test_attention_mask = X_test_tokenized['attention_mask']\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [train_input_ids, train_attention_mask],\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=([test_input_ids, test_attention_mask], y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 5s 328ms/step\n",
      "[[36  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 17  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0 27  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0 39  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 66  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0 16  2  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0 33  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0 27  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 16]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       0.95      1.00      0.98        20\n",
      "           2       0.94      0.94      0.94        18\n",
      "           3       0.96      1.00      0.98        27\n",
      "           4       1.00      0.97      0.99        40\n",
      "           5       1.00      1.00      1.00        14\n",
      "           6       0.97      1.00      0.99        66\n",
      "           7       1.00      1.00      1.00         7\n",
      "           8       1.00      0.84      0.91        19\n",
      "           9       0.94      0.97      0.96        34\n",
      "          10       1.00      0.96      0.98        28\n",
      "          11       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           0.98       325\n",
      "   macro avg       0.98      0.97      0.98       325\n",
      "weighted avg       0.98      0.98      0.98       325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = model.predict([test_input_ids, test_attention_mask])\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)  # Convert one-hot labels to class indices\n",
    "\n",
    "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 17s 409ms/step - loss: 0.0316 - accuracy: 0.9992\n",
      "Test Loss: 0.0316459946334362\n",
      "Test Accuracy: 0.9992307424545288\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(\n",
    "    [train_input_ids, train_attention_mask],  # Model inputs\n",
    "    y_train  # True labels\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "Prediction: ['Brandon Sanderson - Wheel of Time'] vs Actual: Brandon Sanderson - Wheel of Time\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Prediction: ['Brandon Sanderson'] vs Actual: Brandon Sanderson\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Prediction: ['Charles Dickens'] vs Actual: Charles Dickens\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Prediction: ['F Scott Fitzgerald'] vs Actual: F Scott Fitzgerald\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Prediction: ['Fyodor Dostoyevsky'] vs Actual: Fyodor Dostoyevsky\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Prediction: ['Herman Melville'] vs Actual: Herman Melville\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Prediction: ['James Joyce'] vs Actual: James Joyce\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Prediction: ['Mark Twain'] vs Actual: Mark Twain\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Prediction: ['Mary Shelley'] vs Actual: Mary Shelley\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Prediction: ['Oscar Wilde'] vs Actual: Oscar Wilde\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Prediction: ['Robert Jordan'] vs Actual: Robert Jordan\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Prediction: ['Robert Louise Stevenson'] vs Actual: Robert Louise Stevenson\n"
     ]
    }
   ],
   "source": [
    "# Testing model on data not contained within original dataset\n",
    "tests_path = os.path.join(current_dir, \"Tests\")\n",
    "\n",
    "for author in os.listdir(tests_path):\n",
    "    test_file_path = os.path.join(tests_path, author)\n",
    "    \n",
    "    with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "        new_text = file.read()\n",
    "\n",
    "    processed_new_text = preprocess_text(new_text, ps, all_stopwords)\n",
    "    \n",
    "    X_test_tokenized = tokenizer(\n",
    "        processed_new_text, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=330,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict([X_test_tokenized['input_ids'], X_test_tokenized['attention_mask']])\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_classes = predictions.argmax(axis=-1)  # Get the index of the highest probability\n",
    "    \n",
    "    # Map to predicted class to author name\n",
    "    author_names = dataset['Author'].unique()\n",
    "    author_mapping = {index: author for index, author in enumerate(author_names)}\n",
    "    predicted_authors = [author_mapping[i] for i in predicted_classes]\n",
    "    \n",
    "    print(f\"Prediction: {predicted_authors} vs Actual: {author[:-4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual In Depth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        new_text = file.read()\n",
    "    \n",
    "    processed_new_text = preprocess_text(new_text, ps, all_stopwords)\n",
    "    \n",
    "    X_test_tokenized = tokenizer(\n",
    "        processed_new_text, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=330,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict([X_test_tokenized['input_ids'], X_test_tokenized['attention_mask']])\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_classes = predictions.argmax(axis=-1)  # Get the index of the highest probability\n",
    "    \n",
    "    # Map to predicted class to author name\n",
    "    author_names = dataset['Author'].unique()\n",
    "    author_mapping = {index: author for index, author in enumerate(author_names)}\n",
    "    predicted_authors = [author_mapping[i] for i in predicted_classes]\n",
    "    \n",
    "    print(f\"Prediction: {predicted_authors} vs Actual: {author[:-4]}\")\n",
    "    for i,v in enumerate(predictions[0]):\n",
    "        print(f\"{predictions[0][i]} - {author_names[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 243ms/step\n",
      "Prediction: ['Brandon Sanderson - Wheel of Time'] vs Actual: Brandon Sanderson - Wheel of Time\n",
      "0.008072996512055397 - Brandon Sanderson\n",
      "0.8955077528953552 - Brandon Sanderson - Wheel of Time\n",
      "0.00035488069988787174 - Charles Dickens\n",
      "0.0009076691931113601 - F Scott Fitzgerald\n",
      "0.004591451026499271 - Fyodor Dostoyevsky\n",
      "0.0008878319058567286 - Herman Melville\n",
      "0.0004604340356308967 - James Joyce\n",
      "0.0007998549845069647 - Mark Twain\n",
      "0.003405214287340641 - Mary Shelley\n",
      "0.0011477876687422395 - Oscar Wilde\n",
      "0.08293657749891281 - Robert Jordan\n",
      "0.0009276352939195931 - Robert Louise Stevenson\n"
     ]
    }
   ],
   "source": [
    "tests_path = os.path.join(current_dir, \"Tests\")\n",
    "author = \"Brandon Sanderson - Wheel of Time.txt\"\n",
    "test_file_path = os.path.join(tests_path, author)\n",
    "\n",
    "analysis(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 186ms/step\n",
      "Prediction: ['Robert Jordan'] vs Actual: Brandon Sanderson - Wheel of Time\n",
      "0.21989458799362183 - Brandon Sanderson\n",
      "0.051057714968919754 - Brandon Sanderson - Wheel of Time\n",
      "0.01803724467754364 - Charles Dickens\n",
      "0.11215777695178986 - F Scott Fitzgerald\n",
      "0.2581261992454529 - Fyodor Dostoyevsky\n",
      "0.011699359863996506 - Herman Melville\n",
      "0.008803945034742355 - James Joyce\n",
      "0.014872521162033081 - Mark Twain\n",
      "0.005505717359483242 - Mary Shelley\n",
      "0.015822462737560272 - Oscar Wilde\n",
      "0.2816060483455658 - Robert Jordan\n",
      "0.00241646240465343 - Robert Louise Stevenson\n"
     ]
    }
   ],
   "source": [
    "epilogue_file_path = os.path.join(current_dir, \"New Text/Misc.txt\")\n",
    "analysis(epilogue_file_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (linux-venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
