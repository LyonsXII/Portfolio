{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Author Prediction - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 15:06:02.121145: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-14 15:06:02.124112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-14 15:06:02.532276: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-14 15:06:03.407544: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-14 15:06:08.522195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # Supress tensorflow warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
       "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
       "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
       "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
       "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
       "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
       "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Go Emotions Dataset.csv\")\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"text\"].values\n",
    "y = dataset.iloc[:, 9:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum each emotion column\n",
    "class_counts = y.sum(axis=0)\n",
    "\n",
    "# Calculate class weights (one over frequency)\n",
    "class_weights = 1 / (class_counts / class_counts.sum())  \n",
    "class_weights /= class_weights.max()\n",
    "\n",
    "# Convert to tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings, needed for BERT tokeniser\n",
    "X = [x for x in X if x.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Michael/Desktop/Web Development Projects/Personal Projects/Portfolio/python-server/linux_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-02-14 15:06:53.689675: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.738179: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.738218: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.743977: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.744018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.744033: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.976491: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.976547: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.976553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-02-14 15:06:53.976577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 15:06:53.976592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5592 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Max length chosen based on data set size after tokenization\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_tokenized = tokenizer(\n",
    "    X_train, \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=50,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "X_test_tokenized = tokenizer(\n",
    "    X_test, \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=50,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pretrained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_model(learning_rate=2e-5, dropout_rate=0.3):\n",
    "    # Define input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(50,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(50,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Define output\n",
    "    bert_output = bert_model([input_ids, attention_mask])\n",
    "    pooled_output = bert_output.pooler_output\n",
    "    \n",
    "    # Add dropout (prevent overfitting)\n",
    "    dropout = Dropout(dropout_rate)(pooled_output)\n",
    "    \n",
    "    # Create classification layers\n",
    "    go_emotion_options = 28\n",
    "    output = Dense(go_emotion_options, activation=\"sigmoid\")(dropout)\n",
    "    \n",
    "    # # Create model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Unfreeze only the last few layers (e.g., last 4)\n",
    "for layer in bert_model.layers[-4:]:  \n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-6\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Create the model\n",
    "model = create_model(learning_rate=learning_rate, dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # Monitor validation loss\n",
    "    patience=2,                 # Stop after 2 epochs with no improvement\n",
    "    restore_best_weights=True   # Restore model weights from the best epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",   # Watch validation loss\n",
    "    factor=0.5,           # Reduce LR by half\n",
    "    patience=2,           # Wait 2 epochs before reducing\n",
    "    min_lr=1e-6,          # Don't go below 1e-6\n",
    "    verbose=1             # Print updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "7000/7000 [==============================] - 575s 79ms/step - loss: 0.1540 - auc: 0.7787 - val_loss: 0.1209 - val_auc: 0.8849 - lr: 5.0000e-06\n",
      "Epoch 2/6\n",
      "7000/7000 [==============================] - 534s 76ms/step - loss: 0.1256 - auc: 0.8715 - val_loss: 0.1163 - val_auc: 0.8976 - lr: 5.0000e-06\n",
      "Epoch 3/6\n",
      "7000/7000 [==============================] - 601s 86ms/step - loss: 0.1188 - auc: 0.8914 - val_loss: 0.1147 - val_auc: 0.9040 - lr: 5.0000e-06\n",
      "Epoch 4/6\n",
      "7000/7000 [==============================] - 540s 77ms/step - loss: 0.1133 - auc: 0.9052 - val_loss: 0.1155 - val_auc: 0.9030 - lr: 5.0000e-06\n",
      "Epoch 5/6\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.1084 - auc: 0.9158  \n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "7000/7000 [==============================] - 556s 79ms/step - loss: 0.1084 - auc: 0.9158 - val_loss: 0.1160 - val_auc: 0.9031 - lr: 5.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f30583b7a00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set epochs lower if not using early stopping, observed some good results at 5+\n",
    "# Converges at around 19 with seven authors, takes 1h30m on CPU\n",
    "# Lower batch size if running on GPU and you get an out of memory error, 8 seems to work for 8gb VRAM\n",
    "epochs = 6\n",
    "batch_size = 8\n",
    "\n",
    "# Label input ids and attention mask for BERT (from BERT tokenizer), for the sake of convienience\n",
    "train_input_ids = X_tokenized['input_ids']\n",
    "train_attention_mask = X_tokenized['attention_mask']\n",
    "test_input_ids = X_test_tokenized['input_ids']\n",
    "test_attention_mask = X_test_tokenized['attention_mask']\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [train_input_ids, train_attention_mask],\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=([test_input_ids, test_attention_mask], y_test),\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 34s 75ms/step\n",
      "[[ 632   42   17   24   57   15    7   17    4    7   16   12    4   15\n",
      "     5   76    0   32   74    1   30    0    1    0    3   11   32  256]\n",
      " [  17  395    6   11    6    4    3    3    3    2    6    4    1    3\n",
      "     2   15    0   18    8    0    6    0    1    0    2    5   11   58]\n",
      " [   7    5  185   85    9    7    3   10    6   10   22   20    4    1\n",
      "     6    4    0    3    2    0    8    0    0    0    0   13    4  102]\n",
      " [  11   19   59  146   27   17   21   15    2   20   54   29    8    3\n",
      "    11   11    0    6   17    0    8    0    1    0    3   22   15  229]\n",
      " [  81   19    5   24  192   25   14    8    2   10   35    8    1   10\n",
      "    14   31    0   31   40    0   37    0    6    0    6   13    4  408]\n",
      " [  10    1    5    5    7   93    3    4    4    1   10    0    2    1\n",
      "     1   13    0   17    8    0   30    0    0    0    7   21    1   89]\n",
      " [   9    6    4   10    9    1  143   73    0   10   13    4    0    3\n",
      "     2    7    0    1    6    0    7    0    2    0    5    2   14  115]\n",
      " [  18    7    8   10    8    1   71  170    2    6    1    3    2    9\n",
      "     3   11    0    2    7    0    6    0    1    0    3    3   26  125]\n",
      " [   7    3    3    6   14    6    1    5   42    2    2    1    0    4\n",
      "     3    7    0    3    8    0   26    0    0    0    1    5    1   53]\n",
      " [   8    6   12   36   11    8    9    2    1   75   33    8    4    0\n",
      "    13    9    0    4    3    0    4    0    1    0    7   68   10  125]\n",
      " [  13   13   16   34   21    5   25    9    2   21  153   19    1    4\n",
      "     5   10    0    5    9    0    5    0    1    0    2   11    9  208]\n",
      " [   3    4   20   27    3    1    2    2    0   10    7   48    3    0\n",
      "     8    0    0    0    2    0    2    0    0    0    1    6    1   53]\n",
      " [   1    3    3   15    4    1    2    2    2    4    8    9   17    0\n",
      "     7    0    0    0    2    0    1    0    3    0    6    7    0   25]\n",
      " [  32    7    1    4    5    4    3    8    1    1    1    0    0   41\n",
      "     4   14    0   39   20    0   13    0    3    0    0    3   18   67]\n",
      " [   3    2    3    5    0    4    5    0    0    5    5    9    0    0\n",
      "    68    3    0    0    2    0    5    0    0    0    2    7    2   35]\n",
      " [  41    4    2    2    7   13    1    2    1    0    3    0    0    2\n",
      "     0  437    0   28    6    0    9    0    0    0    5    4    2   11]\n",
      " [   0    0    0    3    1    1    0    0    1    3    1    0    0    0\n",
      "     2    0    0    1    0    0    0    0    0    0    0   18    0    7]\n",
      " [  37   34    1    1    6    1    1    2    3    2    3    1    0    5\n",
      "     2    4    0   96   30    0   11    0    0    0    1    3    5   55]\n",
      " [  29    3    1    0    7    8    4    2    1    1    1    0    1    2\n",
      "     3    0    0    3  256    0    3    0    0    0    0    2    0   20]\n",
      " [   0    0    1    1    1    5    2    2    0    4    0    0    0    0\n",
      "    11    0    0    0    0    1    1    0    0    0    0    9    4   24]\n",
      " [  15    9    1    3   17   14    7    0    4    1    5    1    1    4\n",
      "     0    3    0    2    5    0  118    0    0    0    0    4    7   95]\n",
      " [  17    2    0    2    1    4    0    0    0    0    0    0    0    0\n",
      "     0    0    0    7    0    0    1    0    0    0    0    0    0   24]\n",
      " [   7    5    2   10   16    1   10    3    1    4   18    1    1    2\n",
      "     3    1    0    1    6    0    2    0   20    0    5   10   13  159]\n",
      " [   1    2    0    0    9    4    0    0    1    0    0    1    0    1\n",
      "     1    5    0    8    0    0    1    0    0    0    0    0    0   29]\n",
      " [   0    2    0    3    1    0    1    1    0    4    1    2    0    0\n",
      "     0    0    0    0    1    0    3    0    1    0   37   28    0    9]\n",
      " [   0    5    6    6    4    6    0    2    1   18   10    3    0    0\n",
      "     6    1    0    2    1    0    2    0    1    0   17  110    2   54]\n",
      " [  10    9    6   13    0    2   13    6    0    3    7    1    0    1\n",
      "     2    2    0    1    2    0    1    0    1    0    1    0   97   47]\n",
      " [ 174   82  108  135  167   65  145  127   16   51  163   40    4   30\n",
      "    30   14    0   24   55    0   51    0    9    0    8   53   46 2158]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.45      0.49      1390\n",
      "           1       0.57      0.67      0.62       590\n",
      "           2       0.39      0.36      0.37       516\n",
      "           3       0.24      0.19      0.21       754\n",
      "           4       0.31      0.19      0.24      1024\n",
      "           5       0.29      0.28      0.29       333\n",
      "           6       0.29      0.32      0.30       446\n",
      "           7       0.36      0.34      0.35       503\n",
      "           8       0.42      0.21      0.28       203\n",
      "           9       0.27      0.16      0.20       457\n",
      "          10       0.26      0.25      0.26       601\n",
      "          11       0.21      0.24      0.22       203\n",
      "          12       0.31      0.14      0.19       122\n",
      "          13       0.29      0.14      0.19       289\n",
      "          14       0.32      0.41      0.36       165\n",
      "          15       0.64      0.75      0.69       580\n",
      "          16       0.00      0.00      0.00        38\n",
      "          17       0.29      0.32      0.30       304\n",
      "          18       0.45      0.74      0.56       347\n",
      "          19       0.50      0.02      0.03        66\n",
      "          20       0.30      0.37      0.33       316\n",
      "          21       0.00      0.00      0.00        58\n",
      "          22       0.38      0.07      0.11       301\n",
      "          23       0.00      0.00      0.00        63\n",
      "          24       0.30      0.39      0.34        94\n",
      "          25       0.25      0.43      0.32       257\n",
      "          26       0.30      0.43      0.35       225\n",
      "          27       0.47      0.57      0.51      3755\n",
      "\n",
      "    accuracy                           0.41     14000\n",
      "   macro avg       0.32      0.30      0.29     14000\n",
      "weighted avg       0.40      0.41      0.39     14000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Michael/Desktop/Web Development Projects/Personal Projects/Portfolio/python-server/linux_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/Users/Michael/Desktop/Web Development Projects/Personal Projects/Portfolio/python-server/linux_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/Users/Michael/Desktop/Web Development Projects/Personal Projects/Portfolio/python-server/linux_venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = model.predict([test_input_ids, test_attention_mask])\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)  # Convert one-hot labels to class indices\n",
    "\n",
    "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750/1750 [==============================] - 128s 72ms/step - loss: 0.1044 - auc: 0.9260\n",
      "Test Loss: 0.10440154373645782\n",
      "Test Accuracy: 0.9260385036468506\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(\n",
    "    [train_input_ids, train_attention_mask],  # Model inputs\n",
    "    y_train  # True labels\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        new_text = file.read()\n",
    "    \n",
    "    new_text_tokenized = tokenizer(\n",
    "        new_text, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        max_length=50,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict([new_text_tokenized['input_ids'], new_text_tokenized['attention_mask']])\n",
    "    \n",
    "    # Since we're predicting one sample, extract the first (and only) result\n",
    "    predicted_probabilities = predictions[0]\n",
    "    print(\"Predicted probabilities:\", predicted_probabilities)\n",
    "    \n",
    "    # If you have a dictionary mapping indices to emotion names, use it to list the predicted emotions.\n",
    "    # For example, assuming a mapping like this:\n",
    "    emotion_labels = {\n",
    "        0: \"admiration\", 1: \"amusement\", 2: \"anger\", 3: \"annoyance\",\n",
    "        4: \"approval\", 5: \"caring\", 6: \"confusion\", 7: \"curiosity\",\n",
    "        8: \"desire\", 9: \"disappointment\", 10: \"disapproval\", 11: \"disgust\",\n",
    "        12: \"embarrassment\", 13: \"excitement\", 14: \"fear\", 15: \"gratitude\",\n",
    "        16: \"grief\", 17: \"joy\", 18: \"love\", 19: \"nervousness\",\n",
    "        20: \"optimism\", 21: \"pride\", 22: \"realization\", 23: \"relief\",\n",
    "        24: \"remorse\", 25: \"sadness\", 26: \"surprise\", 27: \"neutral\"\n",
    "    }\n",
    "    \n",
    "    top5_indices = np.argsort(predicted_probabilities)[::-1][:5]\n",
    "    \n",
    "    print(\"Top 5 Emotions:\")\n",
    "    for idx in top5_indices:\n",
    "        emotion_name = emotion_labels[idx]\n",
    "        probability_percent = predicted_probabilities[idx] * 100\n",
    "        print(f\"{emotion_name}: {probability_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 104ms/step\n",
      "Predicted probabilities: [0.00574055 0.00363898 0.01780611 0.04828649 0.02478977 0.00777834\n",
      " 0.03122058 0.00708699 0.00913511 0.10208075 0.02524089 0.01111242\n",
      " 0.01050831 0.01088709 0.01747372 0.00672397 0.00718896 0.00211868\n",
      " 0.0009797  0.01384205 0.01746555 0.0025071  0.04839845 0.00713099\n",
      " 0.00711925 0.03564068 0.02668269 0.43117535]\n",
      "Top 5 Emotions:\n",
      "neutral: 43.12%\n",
      "disappointment: 10.21%\n",
      "realization: 4.84%\n",
      "annoyance: 4.83%\n",
      "sadness: 3.56%\n"
     ]
    }
   ],
   "source": [
    "selected_file = \"Wheel of Time - Epilogue\"\n",
    "\n",
    "def load_text(path):\n",
    "    current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    file_path = os.path.join(current_dir, path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "file = \"New Text/\" + selected_file + \".txt\"\n",
    "\n",
    "analysis(file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (linux-venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
