{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Author Prediction - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 19:36:36.706294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-15 19:36:36.708338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-15 19:36:36.954476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-15 19:36:37.455515: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-15 19:36:41.875597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/c/Users/Michael/Desktop/Web Development Projects/Personal Projects/Portfolio/python-server/linux_venv/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # Supress tensorflow warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'split', 'V', 'A', 'D', 'text'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"EmoBank Dataset.csv\")\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['text'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"text\"].values\n",
    "y = dataset.iloc[:, 2:5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings, needed for BERT tokeniser\n",
    "X = [x for x in X if x.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean VAD scores: [2.97921223 3.04346918 3.06416377]\n",
      "Min VAD scores: [1.2 1.8 2. ]\n",
      "Max VAD scores: [4.6 4.4 4.2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean VAD scores:\", np.mean(y_train, axis=0))\n",
    "print(\"Min VAD scores:\", np.min(y_train, axis=0))\n",
    "print(\"Max VAD scores:\", np.max(y_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Michael/Desktop/Web Development Projects/Personal Projects/Portfolio/python-server/linux_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-02-15 19:37:20.483467: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.509838: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.509881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.513259: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.513302: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.513320: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.670829: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.670884: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.670890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-02-15 19:37:20.670915: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-15 19:37:20.670927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5592 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Max length chosen based on data set size after tokenization\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_tokenized = tokenizer(\n",
    "    X_train, \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=70,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "X_test_tokenized = tokenizer(\n",
    "    X_test, \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=70,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    # Compute element-wise squared errors; shape: (batch_size, 3)\n",
    "    errors = tf.square(y_true - y_pred)\n",
    "    \n",
    "    # Compute the mean of y_true for each output dimension; shape: (1, 3)\n",
    "    mean_y_true = tf.reduce_mean(y_true, axis=0, keepdims=True)\n",
    "    \n",
    "    # Compute weights for each element: 1 + abs(y_true - mean_y_true)\n",
    "    # This gives a tensor of shape: (batch_size, 3)\n",
    "    weights = 1 + tf.abs(y_true - mean_y_true)\n",
    "\n",
    "    # Multiply element-wise errors by weights\n",
    "    weighted_errors = errors * weights  # shape: (batch_size, 3)\n",
    "    \n",
    "    # Return the mean of all weighted errors as a scalar\n",
    "    return tf.reduce_mean(weighted_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pretrained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_model(learning_rate=2e-5, dropout_rate=0.3):\n",
    "    # Define input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(70,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(70,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Define output\n",
    "    bert_output = bert_model([input_ids, attention_mask])\n",
    "    pooled_output = bert_output.pooler_output\n",
    "    \n",
    "    # Add dropout (prevent overfitting)\n",
    "    dropout = Dropout(dropout_rate)(pooled_output)\n",
    "\n",
    "    # Create classification layers\n",
    "    emoBank_options = 3\n",
    "    output = Dense(emoBank_options, activation=\"linear\")(dropout)\n",
    "    \n",
    "    # # Create model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=weighted_mse,\n",
    "        metrics=[\"mse\", \"mae\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Unfreeze only the last few layers (e.g., last 4)\n",
    "for layer in bert_model.layers[:1]:  \n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose between a fixed learning rate and CLR (cyclical, bounces between two rates specified)\n",
    "learning_rate = 3e-5\n",
    "# learning rate = tfa.optimizers.CyclicalLearningRate(\n",
    "#     initial_learning_rate=3e-6,   # Minimum learning rate\n",
    "#     maximal_learning_rate=3e-5,   # Maximum learning rate\n",
    "#     step_size=2000,               # Steps to reach max_lr before decreasing\n",
    "#     scale_fn=lambda x: 1 / (2.0 ** (x - 1))  # Scaling function (triangular2 policy)\n",
    "# )\n",
    "\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Create the model\n",
    "model = create_model(learning_rate=learning_rate, dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # Monitor validation loss\n",
    "    patience=3,                 # Stop after 2 epochs with no improvement\n",
    "    restore_best_weights=True   # Restore model weights from the best epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 19:37:42.182112: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f4f438d6b60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-15 19:37:42.182139: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n",
      "2025-02-15 19:37:42.232833: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-15 19:37:42.285492: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739648262.354705   84249 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 94s 276ms/step - loss: 0.4856 - mse: 0.3887 - mae: 0.3961 - val_loss: 0.0717 - val_mse: 0.0534 - val_mae: 0.1769\n",
      "Epoch 2/25\n",
      "252/252 [==============================] - 68s 268ms/step - loss: 0.1084 - mse: 0.0829 - mae: 0.2256 - val_loss: 0.0634 - val_mse: 0.0458 - val_mae: 0.1624\n",
      "Epoch 3/25\n",
      "252/252 [==============================] - 68s 270ms/step - loss: 0.0886 - mse: 0.0682 - mae: 0.2052 - val_loss: 0.0634 - val_mse: 0.0476 - val_mae: 0.1679\n",
      "Epoch 4/25\n",
      "252/252 [==============================] - 67s 267ms/step - loss: 0.0771 - mse: 0.0599 - mae: 0.1926 - val_loss: 0.0583 - val_mse: 0.0425 - val_mae: 0.1564\n",
      "Epoch 5/25\n",
      "252/252 [==============================] - 67s 267ms/step - loss: 0.0703 - mse: 0.0551 - mae: 0.1846 - val_loss: 0.0625 - val_mse: 0.0461 - val_mae: 0.1655\n",
      "Epoch 6/25\n",
      "252/252 [==============================] - 68s 269ms/step - loss: 0.0650 - mse: 0.0513 - mae: 0.1787 - val_loss: 0.0614 - val_mse: 0.0451 - val_mae: 0.1628\n",
      "Epoch 7/25\n",
      "252/252 [==============================] - 65s 257ms/step - loss: 0.0601 - mse: 0.0478 - mae: 0.1731 - val_loss: 0.0627 - val_mse: 0.0474 - val_mae: 0.1667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5082dce260>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# Label input ids and attention mask for BERT (from BERT tokenizer), for the sake of convienience\n",
    "train_input_ids = X_tokenized['input_ids']\n",
    "train_attention_mask = X_tokenized['attention_mask']\n",
    "test_input_ids = X_test_tokenized['input_ids']\n",
    "test_attention_mask = X_test_tokenized['attention_mask']\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [train_input_ids, train_attention_mask],\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=([test_input_ids, test_attention_mask], y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 24s 97ms/step - loss: 0.0360 - mse: 0.0271 - mae: 0.1272\n",
      "Test Loss: 0.03597669675946236\n",
      "Test MSE: 0.027147093787789345\n",
      "Test MAE: 0.12716332077980042\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(\n",
    "    [train_input_ids, train_attention_mask],  # Model inputs\n",
    "    y_train  # True labels\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {result[0]}\")\n",
    "print(f\"Test MSE: {result[1]}\")\n",
    "print(f\"Test MAE: {result[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        new_text = file.read()\n",
    "    \n",
    "    new_text_tokenized = tokenizer(\n",
    "        new_text, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        max_length=50,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict([new_text_tokenized['input_ids'], new_text_tokenized['attention_mask']])\n",
    "    \n",
    "    # Since we're predicting one sample, extract the first (and only) result\n",
    "    predicted_probabilities = predictions[0]\n",
    "    print(\"Valence: \", predicted_probabilities[0])\n",
    "    print(\"Arousal: \", predicted_probabilities[1])\n",
    "    print(\"Dominance: \", predicted_probabilities[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "Valence:  2.94784\n",
      "Arousal:  3.0081213\n",
      "Dominance:  2.984594\n"
     ]
    }
   ],
   "source": [
    "selected_file = \"Veins of Gold\"\n",
    "\n",
    "def load_text(path):\n",
    "    current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    file_path = os.path.join(current_dir, path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "file = \"New Text/\" + selected_file + \".txt\"\n",
    "\n",
    "analysis(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "📝 **High Valence**\n",
      "   🟢 Valence:  4.2690\n",
      "   🔴 Arousal:  3.8780\n",
      "   🔵 Dominance: 3.4783\n",
      "\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "📝 **High Arousal**\n",
      "   🟢 Valence:  4.3619\n",
      "   🔴 Arousal:  4.4137\n",
      "   🔵 Dominance: 3.4335\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "📝 **High Dominance**\n",
      "   🟢 Valence:  3.1962\n",
      "   🔴 Arousal:  3.1353\n",
      "   🔵 Dominance: 3.3430\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "📝 **Low Valence**\n",
      "   🟢 Valence:  2.0306\n",
      "   🔴 Arousal:  3.1616\n",
      "   🔵 Dominance: 2.4815\n",
      "\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "📝 **Low Arousal**\n",
      "   🟢 Valence:  2.8379\n",
      "   🔴 Arousal:  2.8326\n",
      "   🔵 Dominance: 2.8743\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "📝 **Low Dominance**\n",
      "   🟢 Valence:  2.0552\n",
      "   🔴 Arousal:  3.2458\n",
      "   🔵 Dominance: 2.6137\n",
      "\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "📝 **High VAD (Excited Power)**\n",
      "   🟢 Valence:  3.4171\n",
      "   🔴 Arousal:  4.2227\n",
      "   🔵 Dominance: 3.5426\n",
      "\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "📝 **Low Valence, High Arousal, Low Dominance**\n",
      "   🟢 Valence:  2.2353\n",
      "   🔴 Arousal:  3.9827\n",
      "   🔵 Dominance: 3.1016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define sanity check examples with expected outcomes\n",
    "test_sentences = {\n",
    "    \"High Valence\": \"I feel absolutely wonderful today! Everything is going perfectly.\",\n",
    "    \"High Arousal\": \"I can't believe it! This is the most exciting moment of my life!\",\n",
    "    \"High Dominance\": \"I am the leader here. Everyone follows my commands.\",\n",
    "    \"Low Valence\": \"Everything is falling apart. I feel empty and hopeless.\",\n",
    "    \"Low Arousal\": \"It's just another slow and uneventful day at work.\",\n",
    "    \"Low Dominance\": \"I feel so small and helpless in this overwhelming situation.\",\n",
    "    \"High VAD (Excited Power)\": \"I just won the championship! I feel unstoppable!\",\n",
    "    \"Low Valence, High Arousal, Low Dominance\": \"I'm trapped and panicking! There's no escape!\"\n",
    "}\n",
    "\n",
    "def sanity_check_model(model, tokenizer, test_sentences):\n",
    "    for label, sentence in test_sentences.items():\n",
    "        # Tokenize the input sentence\n",
    "        encoded_input = tokenizer(\n",
    "            sentence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=50,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        \n",
    "        # Run prediction\n",
    "        prediction = model.predict([encoded_input[\"input_ids\"], encoded_input[\"attention_mask\"]])\n",
    "        \n",
    "        # Extract scores\n",
    "        valence, arousal, dominance = prediction[0]  # Unpack first prediction\n",
    "\n",
    "        # Print results\n",
    "        print(f\"📝 **{label}**\")\n",
    "        print(f\"   🟢 Valence:  {valence:.4f}\")\n",
    "        print(f\"   🔴 Arousal:  {arousal:.4f}\")\n",
    "        print(f\"   🔵 Dominance: {dominance:.4f}\\n\")\n",
    "        \n",
    "# Run the sanity check\n",
    "sanity_check_model(model, tokenizer, test_sentences)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (linux-venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
