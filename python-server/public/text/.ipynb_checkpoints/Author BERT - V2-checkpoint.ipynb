{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Author Prediction - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Michael\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "root = os.path.join(current_dir, \"Books\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through author and book folders, adding .txt file contents to data with matching labels\n",
    "for author in os.listdir(root):\n",
    "    author_path = os.path.join(root, author)\n",
    "\n",
    "    for book in os.listdir(author_path):\n",
    "        book_path = os.path.join(author_path, book)\n",
    "\n",
    "        for chapter in os.listdir(book_path):\n",
    "            chapter_path = os.path.join(book_path, chapter)\n",
    "            \n",
    "            with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "            data.append({\n",
    "                'Author': author,\n",
    "                'Book': book,\n",
    "                'Chapter': chapter[:-4],\n",
    "                'Text': text\n",
    "            })\n",
    "\n",
    "dataset = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "fragment_size = 200\n",
    "overlap = 50\n",
    "\n",
    "# First we clean our text, converting to lower case and removing unwanted characted\n",
    "# Then we apply corpus, simplifying our text\n",
    "# Finally we split our text into fragments of 'fragment_size', with an overlap of 'overlap' words from the previous fragment\n",
    "\n",
    "def preprocess_text(text, ps, all_stopwords):\n",
    "    # Clean text\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\n', \" \", text)  # Newlines\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', \" \", text)  # Punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Extra spaces\n",
    "\n",
    "    # Apply corpus\n",
    "    words = text.split()\n",
    "    words = [ps.stem(word) for word in words if word not in all_stopwords]\n",
    "    processed_text = \" \".join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def fragment_text(text, fragment_size, overlap):\n",
    "    # Split text into fragments of fragment_size length, returns array of fragments\n",
    "    words = text.split()\n",
    "    current_text_fragments = []\n",
    "    \n",
    "    step_size = fragment_size - overlap  \n",
    "    \n",
    "    for i in range(0, len(words), step_size):\n",
    "        current_fragment = \" \".join(words[i:i + fragment_size])\n",
    "        current_text_fragments.append(current_fragment)\n",
    "\n",
    "        # Handle situation where final chapter fragment is already contained in the previous fragment\n",
    "        if len(words) - i < fragment_size:\n",
    "            break\n",
    "        \n",
    "    return current_text_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "# Apply our cleaning and create a new dataset to replace our previous one, this time with processed text\n",
    "text_fragments = []\n",
    "for index, row in dataset.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    text = preprocess_text(text, ps, all_stopwords)\n",
    "    current_text_fragments = fragment_text(text, fragment_size, overlap)\n",
    "    \n",
    "    for text_fragment in current_text_fragments:\n",
    "        text_fragments.append({\n",
    "            \"Book\": row[\"Book\"],\n",
    "            \"Author\": row[\"Author\"],\n",
    "            \"Text\": text_fragment\n",
    "        })\n",
    "\n",
    "# Convert the data fragments into a Pandas DataFrame and replace the original\n",
    "dataset = pd.DataFrame(text_fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"Text\"].values\n",
    "y = dataset[\"Author\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Reshape y to a 2D array (needed for OneHotEncoder)\n",
    "y_reshaped = y.reshape(-1, 1)\n",
    "\n",
    "# Create and fit OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Creating class weights (in order to reduce overfitting from imbalanced data size per author)\n",
    "y_class_indices = np.argmax(y_encoded, axis=-1)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(y_class_indices), \n",
    "    y=y_class_indices\n",
    ")\n",
    "\n",
    "# Creating class weights dictionary\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings after .strip() applied\n",
    "# There shouldn't be any but BERT tokeniser fails if not done\n",
    "X = [x for x in X if x.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Max length chosen based on token distribution coming from fragment size to minimise truncation\n",
    "# Edit if changing fragment_size, or optionally remove  entirely though this should be less efficient\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_tokenized = tokenizer(\n",
    "    X_train, \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=330,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "X_test_tokenized = tokenizer(\n",
    "    X_test, \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=330,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Michael\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pretrained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_model(learning_rate=2e-5, dropout_rate=0.3):\n",
    "    # Define input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(330,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(330,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Define output\n",
    "    bert_output = bert_model([input_ids, attention_mask])\n",
    "    pooled_output = bert_output.pooler_output\n",
    "    \n",
    "    # Add dropout (prevent overfitting)\n",
    "    dropout = Dropout(dropout_rate)(pooled_output)\n",
    "    \n",
    "    # Create classification layers\n",
    "    num_authors = len(dataset['Author'].unique())\n",
    "    output = Dense(num_authors, activation=\"softmax\")(dropout)\n",
    "    \n",
    "    # # Create model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Unfreeze only the last few layers (e.g., last 4)\n",
    "for layer in bert_model.layers[-4:]:  \n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# Create the model\n",
    "model = create_model(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # Monitor validation loss\n",
    "    patience=3,                 # Stop after 3 epochs with no improvement\n",
    "    restore_best_weights=True   # Restore model weights from the best epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "41/41 [==============================] - 352s 9s/step - loss: 1.9403 - accuracy: 0.2217 - val_loss: 1.7989 - val_accuracy: 0.2927\n",
      "Epoch 2/50\n",
      "41/41 [==============================] - 358s 9s/step - loss: 1.6764 - accuracy: 0.3685 - val_loss: 1.3043 - val_accuracy: 0.6098\n",
      "Epoch 3/50\n",
      "41/41 [==============================] - 355s 9s/step - loss: 1.0660 - accuracy: 0.6636 - val_loss: 0.6322 - val_accuracy: 0.8720\n",
      "Epoch 4/50\n",
      "41/41 [==============================] - 330s 8s/step - loss: 0.4731 - accuracy: 0.8914 - val_loss: 0.3289 - val_accuracy: 0.9451\n",
      "Epoch 5/50\n",
      "41/41 [==============================] - 336s 8s/step - loss: 0.2552 - accuracy: 0.9526 - val_loss: 0.2201 - val_accuracy: 0.9634\n",
      "Epoch 6/50\n",
      "41/41 [==============================] - 338s 8s/step - loss: 0.1334 - accuracy: 0.9801 - val_loss: 0.2032 - val_accuracy: 0.9451\n",
      "Epoch 7/50\n",
      "41/41 [==============================] - 339s 8s/step - loss: 0.0933 - accuracy: 0.9832 - val_loss: 0.1571 - val_accuracy: 0.9512\n",
      "Epoch 8/50\n",
      "41/41 [==============================] - 330s 8s/step - loss: 0.0536 - accuracy: 0.9939 - val_loss: 0.2288 - val_accuracy: 0.9329\n",
      "Epoch 9/50\n",
      "41/41 [==============================] - 343s 8s/step - loss: 0.0499 - accuracy: 0.9939 - val_loss: 0.1879 - val_accuracy: 0.9451\n",
      "Epoch 10/50\n",
      "41/41 [==============================] - 327s 8s/step - loss: 0.0276 - accuracy: 0.9969 - val_loss: 0.1133 - val_accuracy: 0.9573\n",
      "Epoch 11/50\n",
      "41/41 [==============================] - 328s 8s/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9695\n",
      "Epoch 12/50\n",
      "41/41 [==============================] - 328s 8s/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 0.9756\n",
      "Epoch 13/50\n",
      "41/41 [==============================] - 327s 8s/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0860 - val_accuracy: 0.9695\n",
      "Epoch 14/50\n",
      "41/41 [==============================] - 328s 8s/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0915 - val_accuracy: 0.9756\n",
      "Epoch 15/50\n",
      "41/41 [==============================] - 327s 8s/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0951 - val_accuracy: 0.9695\n",
      "Epoch 16/50\n",
      "41/41 [==============================] - 327s 8s/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9756\n",
      "Epoch 17/50\n",
      "41/41 [==============================] - 328s 8s/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9756\n",
      "Epoch 18/50\n",
      "41/41 [==============================] - 327s 8s/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0880 - val_accuracy: 0.9756\n",
      "Epoch 19/50\n",
      "41/41 [==============================] - 327s 8s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x280130c4510>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set epochs lower if not using early stopping, observed some good results at 5+\n",
    "# Converges at around 19 with seven authors, takes 1h30m on CPU\n",
    "epochs = 50 \n",
    "batch_size = 16\n",
    "\n",
    "# Label input ids and attention mask for BERT (from BERT tokenizer), for the sake of convienience\n",
    "train_input_ids = X_tokenized['input_ids']\n",
    "train_attention_mask = X_tokenized['attention_mask']\n",
    "test_input_ids = X_test_tokenized['input_ids']\n",
    "test_attention_mask = X_test_tokenized['attention_mask']\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [train_input_ids, train_attention_mask],\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=([test_input_ids, test_attention_mask], y_test),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 24s 4s/step\n",
      "[[20  0  1  0  0  0  1]\n",
      " [ 0 29  0  0  0  0  0]\n",
      " [ 0  0 29  0  1  0  0]\n",
      " [ 0  0  0 14  0  0  0]\n",
      " [ 0  0  0  0 26  0  0]\n",
      " [ 0  0  0  0  0 31  0]\n",
      " [ 0  0  0  0  1  0 11]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        22\n",
      "           1       1.00      1.00      1.00        29\n",
      "           2       0.97      0.97      0.97        30\n",
      "           3       1.00      1.00      1.00        14\n",
      "           4       0.93      1.00      0.96        26\n",
      "           5       1.00      1.00      1.00        31\n",
      "           6       0.92      0.92      0.92        12\n",
      "\n",
      "    accuracy                           0.98       164\n",
      "   macro avg       0.97      0.97      0.97       164\n",
      "weighted avg       0.98      0.98      0.98       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = model.predict([test_input_ids, test_attention_mask])\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)  # Convert one-hot labels to class indices\n",
    "\n",
    "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 90s 4s/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Test Loss: 0.002359158592298627\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(\n",
    "    [train_input_ids, train_attention_mask],  # Model inputs\n",
    "    y_train  # True labels\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 134ms/step\n",
      "Prediction: ['Charles Dickens'] vs Actual: Charles Dickens\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "Prediction: ['F Scott Fitzgerald'] vs Actual: F Scott Fitzgerald\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "Prediction: ['Fyodor Dostoyevsky'] vs Actual: Fyodor Dostoyevsky\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "Prediction: ['Herman Melville'] vs Actual: Herman Melville\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "Prediction: ['Mary Shelley'] vs Actual: Mary Shelley\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "Prediction: ['Oscar Wilde'] vs Actual: Oscar Wilde\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "Prediction: ['Robert Louise Stevenson'] vs Actual: Robert Louise Stevenson\n"
     ]
    }
   ],
   "source": [
    "# Testing model on data not contained within original dataset\n",
    "tests_path = os.path.join(current_dir, \"Tests\")\n",
    "\n",
    "for author in os.listdir(tests_path):\n",
    "    test_file_path = os.path.join(tests_path, author)\n",
    "    \n",
    "    with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "        new_text = file.read()\n",
    "\n",
    "    processed_new_text = preprocess_text(new_text, ps, all_stopwords)\n",
    "    \n",
    "    X_test_tokenized = tokenizer(\n",
    "        processed_new_text, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=330,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict([X_test_tokenized['input_ids'], X_test_tokenized['attention_mask']])\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_classes = predictions.argmax(axis=-1)  # Get the index of the highest probability\n",
    "    \n",
    "    # Map to predicted class to author name\n",
    "    author_names = dataset['Author'].unique()\n",
    "    author_mapping = {index: author for index, author in enumerate(author_names)}\n",
    "    predicted_authors = [author_mapping[i] for i in predicted_classes]\n",
    "    \n",
    "    print(f\"Prediction: {predicted_authors} vs Actual: {author[:-4]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
