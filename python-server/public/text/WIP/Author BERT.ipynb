{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "root = os.path.join(current_dir, \"Books\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through author and book folders, adding .txt file contents to data with matching labels\n",
    "for author in os.listdir(root):\n",
    "    author_path = os.path.join(root, author)\n",
    "\n",
    "    for book in os.listdir(author_path):\n",
    "        book_path = os.path.join(author_path, book)\n",
    "\n",
    "        for chapter in os.listdir(book_path):\n",
    "            chapter_path = os.path.join(book_path, chapter)\n",
    "            \n",
    "            with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "            data.append({\n",
    "                'Author': author,\n",
    "                'Book': book,\n",
    "                'Chapter': chapter[:-4],\n",
    "                'Text': text\n",
    "            })\n",
    "\n",
    "dataset = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "fragment_size = 200\n",
    "overlap = 50\n",
    "\n",
    "# First we clean our text, converting to lower case and removing unwanted characted\n",
    "# Then we apply corpus, simplifying our text\n",
    "# Finally we split our text into fragments of 'fragment_size', with an overlap of 'overlap' words from the previous fragment\n",
    "\n",
    "def preprocess_text(text, ps, all_stopwords):\n",
    "    # Clean text\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\n', \" \", text)  # Newlines\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', \" \", text)  # Punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Extra spaces\n",
    "\n",
    "    # Apply corpus\n",
    "    words = text.split()\n",
    "    words = [ps.stem(word) for word in words if word not in all_stopwords]\n",
    "    processed_text = \" \".join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def fragment_text(text, fragment_size, overlap):\n",
    "    # Split text into fragments of fragment_size length, returns array of fragments\n",
    "    words = text.split()\n",
    "    current_text_fragments = []\n",
    "    \n",
    "    step_size = fragment_size - overlap  \n",
    "    \n",
    "    for i in range(0, len(words), step_size):\n",
    "        current_fragment = \" \".join(words[i:i + fragment_size])\n",
    "        current_text_fragments.append(current_fragment)\n",
    "\n",
    "        # Handle situation where final chapter fragment is already contained in the previous fragment\n",
    "        if len(words) - i < fragment_size:\n",
    "            break\n",
    "        \n",
    "    return current_text_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "# Apply our cleaning and create a new dataset to replace our previous one, this time with processed text\n",
    "text_fragments = []\n",
    "for index, row in dataset.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    text = preprocess_text(text, ps, all_stopwords)\n",
    "    current_text_fragments = fragment_text(text, fragment_size, overlap)\n",
    "    \n",
    "    for text_fragment in current_text_fragments:\n",
    "        text_fragments.append({\n",
    "            \"Book\": row[\"Book\"],\n",
    "            \"Author\": row[\"Author\"],\n",
    "            \"Text\": text_fragment\n",
    "        })\n",
    "\n",
    "# Convert the data fragments into a Pandas DataFrame and replace the original\n",
    "dataset = pd.DataFrame(text_fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"Text\"].values\n",
    "y = dataset[\"Author\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Reshape y to a 2D array (needed by OneHotEncoder)\n",
    "y_reshaped = y.reshape(-1, 1)\n",
    "\n",
    "# Create and fit the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Creating class weights (in order to reduce overfitting from imbalanced data size per author)\n",
    "y_class_indices = np.argmax(y_encoded, axis=-1)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(y_class_indices), \n",
    "    y=y_class_indices\n",
    ")\n",
    "\n",
    "# Creating class weights dictionary\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings after .strip() applied\n",
    "# There shouldn't be any but tokeniser fails if not done\n",
    "X = [x for x in X if x.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Max length chosen based on token distribution coming from fragment size to minimise truncation\n",
    "# Edit if changing fragment_size, or remove entirely although this should be less efficient\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "X_tokenized = tokenizer(\n",
    "    X, \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=330,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load pretrained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_model():\n",
    "    # Define input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(330,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(330,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Wrap BERT in a Lambda layer to handle KerasTensors correctly\n",
    "    def bert_layer(inputs):\n",
    "        return bert_model(input_ids=inputs[0], attention_mask=inputs[1])[1]\n",
    "    \n",
    "    bert_output = Lambda(bert_layer, output_shape=(768,))([input_ids, attention_mask])\n",
    "    \n",
    "    # Add dropout (prevent overfitting)\n",
    "    dropout = Dropout(0.3)(bert_output)\n",
    "    \n",
    "    # Create classification layers\n",
    "    num_authors = len(dataset['Author'].unique())\n",
    "    output = Dense(num_authors, activation=\"softmax\")(dropout)\n",
    "    \n",
    "    # # Create model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "# val_predictions = model.predict([X_val_fold['input_ids'], X_val_fold['attention_mask']])\n",
    "# val_pred_classes = val_predictions.argmax(axis=-1)\n",
    "# print(f\"Fold {fold + 1} Classification Report:\")\n",
    "# print(classification_report(np.argmax(y_val_fold, axis=-1), val_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_path = os.path.join(current_dir, \"Tests\")\n",
    "\n",
    "test_file_path = os.path.join(tests_path, \"Oscar Wilde.txt\")\n",
    "\n",
    "with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "    new_text = file.read()\n",
    "\n",
    "processed_new_text = preprocess_text(new_text, ps, all_stopwords)\n",
    "\n",
    "X_test_tokenized = tokenizer(\n",
    "    processed_new_text, \n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=330,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "['Herman Melville']\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.predict([X_test_tokenized['input_ids'], X_test_tokenized['attention_mask']])\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_classes = predictions.argmax(axis=-1)  # Get the index of the highest probability\n",
    "\n",
    "# Map to predicted class to author name\n",
    "author_names = dataset['Author'].unique()\n",
    "author_mapping = {index: author for index, author in enumerate(author_names)}\n",
    "predicted_authors = [author_mapping[i] for i in predicted_classes]\n",
    "\n",
    "print(predicted_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 4s/step - accuracy: 0.0812 - loss: 2.1858    \n",
      "Test Loss: 2.036719560623169\n",
      "Test Accuracy: 0.11369193345308304\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_tokenized is your test data and y are the true labels (one-hot encoded)\n",
    "loss, accuracy = model.evaluate(\n",
    "    [X_tokenized['input_ids'], X_tokenized['attention_mask']],  # Model inputs\n",
    "    y_encoded  # True labels\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate New Text"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
