{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXUkhkMfU4wq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Book        Author  \\\n",
      "0  Frankenstein  Mary Shelley   \n",
      "1  Frankenstein  Mary Shelley   \n",
      "2  Frankenstein  Mary Shelley   \n",
      "3  Frankenstein  Mary Shelley   \n",
      "4  Frankenstein  Mary Shelley   \n",
      "\n",
      "                                                Text  \n",
      "0  To Mrs. Saville, England.\\n\\nSt. Petersburgh, ...  \n",
      "1  To Mrs. Saville, England.\\n\\nArchangel, 28th M...  \n",
      "2  To Mrs. Saville, England.\\n\\nJuly 7th, 17.\\n\\...  \n",
      "3  To Mrs. Saville, England.\\n\\nAugust 5th, 17.\\...  \n",
      "4  I am by birth a Genevese, and my family is one...  \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "dataset = pd.read_csv('Author Dataset.csv', encoding='latin1', sep=',', quoting=csv.QUOTE_ALL)\n",
    "dataset = dataset.drop(\"Chapter\", axis=1)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "fragment_size = 200\n",
    "overlap = 50\n",
    "\n",
    "def preprocess_text(text, ps, all_stopwords):\n",
    "    # Clean text\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\n', \" \", text)  # Newlines\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', \" \", text)  # Punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Extra spaces\n",
    "\n",
    "    # Apply corpus\n",
    "    words = text.split()\n",
    "    words = [ps.stem(word) for word in words if word not in all_stopwords]\n",
    "    processed_text = \" \".join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def fragment_text(text, fragment_size, overlap):\n",
    "    # Split text into fragments of fragment_size length, returns array of fragments\n",
    "    words = text.split()\n",
    "    current_text_fragments = []\n",
    "    \n",
    "    step_size = fragment_size - overlap  \n",
    "    \n",
    "    for i in range(0, len(words), step_size):\n",
    "        current_fragment = \" \".join(words[i:i + fragment_size])\n",
    "        current_text_fragments.append(current_fragment)\n",
    "\n",
    "        # Handle situation where final chapter fragment is already contained in the previous fragment\n",
    "        if len(words) - i < fragment_size:\n",
    "            break\n",
    "        \n",
    "    return current_text_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "text_fragments = []\n",
    "for index, row in dataset.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    text = preprocess_text(text, ps, all_stopwords)\n",
    "    current_text_fragments = fragment_text(text, fragment_size, overlap)\n",
    "    \n",
    "    for text_fragment in current_text_fragments:\n",
    "        text_fragments.append({\n",
    "            \"Book\": row[\"Book\"],\n",
    "            \"Author\": row[\"Author\"],\n",
    "            \"Text\": text_fragment\n",
    "        })\n",
    "\n",
    "# Convert the data fragments into a Pandas DataFrame and replace the original\n",
    "dataset = pd.DataFrame(text_fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"Text\"].values\n",
    "y = dataset[\"Author\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenised = tokenizer.texts_to_sequences(X)\n",
    "# print(\"Word Index:\", tokenizer.word_index)\n",
    "# print(\"Sequences:\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = fragment_size\n",
    "X_padded = pad_sequences(X_tokenised, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array(X_padded)\n",
    "y = np.array(y)\n",
    "\n",
    "# K-fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4908 - loss: 1.0737 - val_accuracy: 0.4930 - val_loss: 1.0299\n",
      "Epoch 2/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4661 - loss: 0.9973 - val_accuracy: 0.4930 - val_loss: 0.9986\n",
      "Epoch 3/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5218 - loss: 0.8537 - val_accuracy: 0.4930 - val_loss: 0.8707\n",
      "Epoch 4/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7585 - loss: 0.6475 - val_accuracy: 0.6761 - val_loss: 0.6523\n",
      "Epoch 5/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9276 - loss: 0.3753 - val_accuracy: 0.8732 - val_loss: 0.3622\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8429 - loss: 0.4391 \n",
      "Epoch 1/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4708 - loss: 1.0755 - val_accuracy: 0.4789 - val_loss: 1.0462\n",
      "Epoch 2/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5229 - loss: 0.9614 - val_accuracy: 0.4789 - val_loss: 1.0184\n",
      "Epoch 3/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5685 - loss: 0.8608 - val_accuracy: 0.4789 - val_loss: 0.9464\n",
      "Epoch 4/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7318 - loss: 0.6661 - val_accuracy: 0.6197 - val_loss: 0.7204\n",
      "Epoch 5/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9674 - loss: 0.3834 - val_accuracy: 0.9155 - val_loss: 0.3842\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8952 - loss: 0.4319 \n",
      "Epoch 1/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4334 - loss: 1.0778 - val_accuracy: 0.4714 - val_loss: 1.0355\n",
      "Epoch 2/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5197 - loss: 0.9776 - val_accuracy: 0.4714 - val_loss: 1.0020\n",
      "Epoch 3/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5125 - loss: 0.8900 - val_accuracy: 0.4714 - val_loss: 0.9146\n",
      "Epoch 4/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6947 - loss: 0.6951 - val_accuracy: 0.5286 - val_loss: 0.7416\n",
      "Epoch 5/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9440 - loss: 0.3872 - val_accuracy: 0.9143 - val_loss: 0.3801\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8868 - loss: 0.4557 \n",
      "Epoch 1/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.3904 - loss: 1.0806 - val_accuracy: 0.5286 - val_loss: 1.0012\n",
      "Epoch 2/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4994 - loss: 0.9930 - val_accuracy: 0.5286 - val_loss: 0.9871\n",
      "Epoch 3/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5862 - loss: 0.8873 - val_accuracy: 0.5286 - val_loss: 0.9189\n",
      "Epoch 4/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6314 - loss: 0.7804 - val_accuracy: 0.5429 - val_loss: 0.7829\n",
      "Epoch 5/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8756 - loss: 0.5435 - val_accuracy: 0.9571 - val_loss: 0.4596\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9434 - loss: 0.5207 \n",
      "Epoch 1/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.4908 - loss: 1.0872 - val_accuracy: 0.5000 - val_loss: 1.0162\n",
      "Epoch 2/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4709 - loss: 1.0096 - val_accuracy: 0.5000 - val_loss: 0.9864\n",
      "Epoch 3/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4464 - loss: 0.9292 - val_accuracy: 0.5000 - val_loss: 0.9118\n",
      "Epoch 4/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5708 - loss: 0.7323 - val_accuracy: 0.5571 - val_loss: 0.7167\n",
      "Epoch 5/5\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8580 - loss: 0.4968 - val_accuracy: 0.8000 - val_loss: 0.4873\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7359 - loss: 0.6138 \n",
      "Average Accuracy: 0.8920\n"
     ]
    }
   ],
   "source": [
    "accuracies = []  # List to store accuracy for each fold\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialise model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=20000, output_dim=64))  # Adjust input_dim based on your vocabulary size\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(np.unique(y_train)), activation='softmax'))  # Number of classes in y_train\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Save the accuracy for this fold\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f'Average Accuracy: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"There was at present nothing to be learned from the Piccadilly side, and nothing could be done; so I went round to the back to see if anything could be gathered from this quarter. The mews were active, the Piccadilly houses being mostly in occupation. I asked one or two of the grooms and helpers whom I saw around if they could tell me anything about the empty house. One of them said that he heard it had lately been taken, but he couldn’t say from whom. He told me, however, that up to very lately there had been a notice-board of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Process text for model\n",
    "processed_new_text = preprocess_text(new_text, ps, all_stopwords)\n",
    "tokenised_new_text = tokenizer.texts_to_sequences([processed_new_text])\n",
    "padded_new_text = pad_sequences(tokenised_new_text, maxlen=fragment_size, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted Author: Lewis Carol\n"
     ]
    }
   ],
   "source": [
    "predicted_class = model.predict(padded_new_text)\n",
    "\n",
    "predicted_class_label = np.argmax(predicted_class, axis=1)\n",
    "author_mapping = {0: \"Mary Shelley\", 1: \"Lewis Carol\", 2: \"Bram Stoker\"}\n",
    "predicted_author = author_mapping[predicted_class_label[0]]\n",
    "print(f\"Predicted Author: {predicted_author}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
