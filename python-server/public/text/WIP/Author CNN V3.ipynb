{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Author                  Book Chapter  \\\n",
      "0  Charles Dickens  A Tale of Two Cities   1 - 1   \n",
      "1  Charles Dickens  A Tale of Two Cities   1 - 2   \n",
      "2  Charles Dickens  A Tale of Two Cities   1 - 3   \n",
      "3  Charles Dickens  A Tale of Two Cities   1 - 4   \n",
      "4  Charles Dickens  A Tale of Two Cities   1 - 5   \n",
      "\n",
      "                                                Text  \n",
      "0   It was the best of times, it was the worst of...  \n",
      "1  It was the Dover road that lay, on a Friday ni...  \n",
      "2  A wonderful fact to reflect upon, that every h...  \n",
      "3  Then the mail got successfully to Dover, in th...  \n",
      "4  A large cask of wine had been dropped and brok...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "root = os.path.join(current_dir, \"Books\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through author and book folders, adding .txt file contents to data with matching labels\n",
    "for author in os.listdir(root):\n",
    "    author_path = os.path.join(root, author)\n",
    "\n",
    "    for book in os.listdir(author_path):\n",
    "        book_path = os.path.join(author_path, book)\n",
    "\n",
    "        for chapter in os.listdir(book_path):\n",
    "            chapter_path = os.path.join(book_path, chapter)\n",
    "            \n",
    "            with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "            data.append({\n",
    "                'Author': author,\n",
    "                'Book': book,\n",
    "                'Chapter': chapter[:-4],\n",
    "                'Text': text\n",
    "            })\n",
    "\n",
    "dataset = pd.DataFrame(data)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "fragment_size = 200\n",
    "overlap = 50\n",
    "\n",
    "def preprocess_text(text, ps, all_stopwords):\n",
    "    # Clean text\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\n', \" \", text)  # Newlines\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', \" \", text)  # Punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Extra spaces\n",
    "\n",
    "    # Apply corpus\n",
    "    words = text.split()\n",
    "    words = [ps.stem(word) for word in words if word not in all_stopwords]\n",
    "    processed_text = \" \".join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def fragment_text(text, fragment_size, overlap):\n",
    "    # Split text into fragments of fragment_size length, returns array of fragments\n",
    "    words = text.split()\n",
    "    current_text_fragments = []\n",
    "    \n",
    "    step_size = fragment_size - overlap  \n",
    "    \n",
    "    for i in range(0, len(words), step_size):\n",
    "        current_fragment = \" \".join(words[i:i + fragment_size])\n",
    "        current_text_fragments.append(current_fragment)\n",
    "\n",
    "        # Handle situation where final chapter fragment is already contained in the previous fragment\n",
    "        if len(words) - i < fragment_size:\n",
    "            break\n",
    "        \n",
    "    return current_text_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "text_fragments = []\n",
    "for index, row in dataset.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    text = preprocess_text(text, ps, all_stopwords)\n",
    "    current_text_fragments = fragment_text(text, fragment_size, overlap)\n",
    "    \n",
    "    for text_fragment in current_text_fragments:\n",
    "        text_fragments.append({\n",
    "            \"Book\": row[\"Book\"],\n",
    "            \"Author\": row[\"Author\"],\n",
    "            \"Text\": text_fragment\n",
    "        })\n",
    "\n",
    "# Convert the data fragments into a Pandas DataFrame and replace the original\n",
    "dataset = pd.DataFrame(text_fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"Text\"].values\n",
    "y = dataset[\"Author\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenised = tokenizer.texts_to_sequences(X)\n",
    "# print(\"Word Index:\", tokenizer.word_index)\n",
    "# print(\"Sequences:\", X_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = fragment_size\n",
    "X_padded = pad_sequences(X_tokenised, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array(X_padded)\n",
    "y = np.array(y)\n",
    "\n",
    "# K-fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1612 - loss: 1.9521 - val_accuracy: 0.1768 - val_loss: 1.9135\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2182 - loss: 1.8715 - val_accuracy: 0.1768 - val_loss: 1.8607\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2826 - loss: 1.6880 - val_accuracy: 0.4085 - val_loss: 1.4993\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5110 - loss: 1.2957 - val_accuracy: 0.6159 - val_loss: 1.1511\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6051 - loss: 0.9532 - val_accuracy: 0.7134 - val_loss: 0.8193\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7447 - loss: 0.8165 \n",
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1821 - loss: 1.9427 - val_accuracy: 0.1707 - val_loss: 1.9319\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2773 - loss: 1.8017 - val_accuracy: 0.2439 - val_loss: 1.8715\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4016 - loss: 1.6102 - val_accuracy: 0.4634 - val_loss: 1.5836\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6422 - loss: 1.1246 - val_accuracy: 0.5976 - val_loss: 1.0096\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7410 - loss: 0.7061 - val_accuracy: 0.7561 - val_loss: 0.6571\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7115 - loss: 0.6727 \n",
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1663 - loss: 1.9465 - val_accuracy: 0.2256 - val_loss: 1.8826\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2486 - loss: 1.8419 - val_accuracy: 0.3476 - val_loss: 1.8034\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3262 - loss: 1.6971 - val_accuracy: 0.3110 - val_loss: 1.5779\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5111 - loss: 1.3138 - val_accuracy: 0.5793 - val_loss: 1.0778\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6560 - loss: 0.8773 - val_accuracy: 0.8293 - val_loss: 0.5983\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8781 - loss: 0.5298 \n",
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1436 - loss: 1.9441 - val_accuracy: 0.2147 - val_loss: 1.8808\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2455 - loss: 1.8587 - val_accuracy: 0.1718 - val_loss: 1.8596\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3239 - loss: 1.7515 - val_accuracy: 0.4172 - val_loss: 1.5845\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4527 - loss: 1.3616 - val_accuracy: 0.4356 - val_loss: 0.9961\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5804 - loss: 0.9258 - val_accuracy: 0.8405 - val_loss: 0.6776\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8856 - loss: 0.5833 \n",
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1722 - loss: 1.9398 - val_accuracy: 0.1963 - val_loss: 1.8882\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2578 - loss: 1.8415 - val_accuracy: 0.2086 - val_loss: 1.8225\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3734 - loss: 1.6703 - val_accuracy: 0.4663 - val_loss: 1.5088\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5976 - loss: 1.1888 - val_accuracy: 0.7117 - val_loss: 0.9319\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7313 - loss: 0.7437 - val_accuracy: 0.8405 - val_loss: 0.5880\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8840 - loss: 0.5201 \n",
      "Average Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "accuracies = []  # List to store accuracy for each fold\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialise model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=20000, output_dim=64))  # Adjust input_dim based on your vocabulary size\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Save the accuracy for this fold\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f'Average Accuracy: {average_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Herman Melville vs Robert Louise Stevenson\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Oscar Wilde vs Charles Dickens\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Robert Louise Stevenson vs Mary Shelley\n",
      "Passed 0 out of 3\n"
     ]
    }
   ],
   "source": [
    "tests_path = os.path.join(current_dir, \"Tests\")\n",
    "\n",
    "count = 0\n",
    "passed = 0\n",
    "for author in os.listdir(tests_path):\n",
    "    test_file_path = os.path.join(tests_path, author)\n",
    "\n",
    "    with open(test_file_path, 'r', encoding='utf-8') as file:\n",
    "        new_text = file.read()\n",
    "\n",
    "    # Process text for model\n",
    "    processed_new_text = preprocess_text(new_text, ps, all_stopwords)\n",
    "    tokenised_new_text = tokenizer.texts_to_sequences([processed_new_text])\n",
    "    padded_new_text = pad_sequences(tokenised_new_text, maxlen=fragment_size, padding='post', truncating='post')\n",
    "\n",
    "    # Make prediction\n",
    "    predicted_class = model.predict(padded_new_text)\n",
    "\n",
    "    predicted_class_label = np.argmax(predicted_class, axis=1)\n",
    "    author_names = dataset['Author'].unique()\n",
    "    author_mapping = {index: author for index, author in enumerate(author_names)}\n",
    "    predicted_author = author_mapping[predicted_class_label[0]]\n",
    "    print(f\"{author[:-4]} vs {predicted_author}\")\n",
    "\n",
    "    # Index counts\n",
    "    count += 1\n",
    "    if author[:-4] == predicted_author:\n",
    "        passed += 1\n",
    "\n",
    "print(f\"Passed {passed} out of {count}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeRFWFoGrdaL5S3dx5MWmb",
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
